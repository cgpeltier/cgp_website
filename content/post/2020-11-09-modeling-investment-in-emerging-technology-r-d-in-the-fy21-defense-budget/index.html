---
title: Modeling Investment in Emerging Technology R&D in the FY21 Defense Budget
author: Chad Peltier
date: '2020-11-09'
slug: modeling-investment-in-emerging-technology-r-d-in-the-fy21-defense-budget
categories: []
tags:
  - NLP
  - Defense
---

<link href="index_files/anchor-sections/anchor-sections.css" rel="stylesheet" />
<script src="index_files/anchor-sections/anchor-sections.js"></script>


<p>This project classifies Planned Programs/Accomplishments in the DoD RDT&amp;E budget based on whether the text of the program is related to emerging technologies or not. This is related to my previous projects on the DoD budget, <a href="https://warontherocks.com/2020/03/put-your-money-where-your-strategy-is-using-machine-learning-to-analyze-the-pentagon-budget/">here</a> and <a href="https://github.com/cgpeltier/Defense/blob/master/dod_budget_scraping_fy2021.md">here</a>.</p>
<p>However, the code below is related to some work I’m doing with a really great team at DataRobot (including Ian Clark, Sarah Khatry, Eric Loeb, and Ted Kwartler) for the National Security Commission on Artificial Intelligence. The NSCAI is producing a report for Congress on U.S. AI strategy, which includes data and visualizations on DoD AI spending. The code below is just an example of the kind of work that we’re doing – the actual models for AI classification for the NSCAI were made using DataRobot’s automated ML platform and use Keras/TensorFlow.</p>
<p>Regardless, this project contains many of the changes that I had wanted to make previously. For example, it uses XMLs of the RDT&amp;E budget instead of scraping the PDFs, like I’d done previously (thanks to Chris Bridge, my coworker at Janes, for that!). It also uses multiple binary classification models instead of a multiclass model.</p>
<p>To standardize our terminology, we decided to call the most specific line item budget allocations in the RDT&amp;E budget “programs”, which are within “projects”, which are themselves under “program elements”. In our dataset, these are abbreviated, “prog_”, “proj_”, and “pe_”, respectively.</p>
<p>The file I read in below contains 600 programs hand-labeled (by Ian, Sarah, and me) as related to emerging tech or not. Note that we’re more or less using the DoD’s <a href="https://www.cto.mil/modernization-priorities/">modernization priorities</a> to determine what is within the broader category of “emerging tech” or not. For a full analysis, we’d then take all of the programs classified as emerging tech by the model and run subsequent one-vs-all binary classification models for each category of emerging tech (i.e. hypersonics, AI, quantum, etc.), but here we’ll just do the broader emerging tech category as an example.</p>
<pre class="r"><code>library(tidyverse)
library(janitor)
library(tidymodels)
library(topicmodels)
library(textrecipes)
library(tidytext)
library(textfeatures)


emerging_tech &lt;- read_csv(&quot;C:\\Users\\chad.peltier\\OneDrive - IHS Markit\\Desktop\\data_projects\\NCSAI_budget_analysis\\scripts\\emerging_tech_training.csv&quot;)

glimpse(emerging_tech)</code></pre>
<pre><code>## Rows: 600
## Columns: 8
## $ budget_year             &lt;dbl&gt; 2021, 2021, 2021, 2021, 2021, 2021, 2021, 2...
## $ project_number          &lt;chr&gt; &quot;6369DF&quot;, &quot;ES1&quot;, &quot;CD1&quot;, &quot;65A006&quot;, &quot;FF2&quot;, &quot;T...
## $ program_element_number  &lt;chr&gt; &quot;0603203F&quot;, &quot;0607134A&quot;, &quot;0608041A&quot;, &quot;120642...
## $ prog_title              &lt;chr&gt; &quot;Sensing Assignments and Multisource Analyt...
## $ prog_description        &lt;chr&gt; &quot;Develop advanced techniques for multi-doma...
## $ prog_current_year_plans &lt;chr&gt; &quot;Develop algorithms to generate and modify ...
## $ prog_year_one_plans     &lt;chr&gt; &quot;For FY 2021, this work is performed under ...
## $ emerging_tech           &lt;dbl&gt; 1, 0, 0, 1, 0, 0, 0, 1, 0, NA, 1, 0, 0, 1, ...</code></pre>
<p>To prep this data for modeling, we’ll do a few things first.</p>
<ul>
<li>Create a unique ID for each program (because unlike PEs and Projects, program line items don’t have unique IDs in the budget) from the PE number, Project number, budget year, and row number.</li>
<li>Unite the text columns together that we’ll tokenize later.</li>
<li>Remove any NAs from the data.</li>
</ul>
<pre class="r"><code>emerging_tech_raw &lt;- emerging_tech %&gt;%
    mutate(row_num = row_number()) %&gt;% 
    unite(col = text, starts_with(&quot;prog_&quot;), sep = &quot; &quot;, na.rm = TRUE, remove = TRUE) %&gt;%
    unite(col = &quot;id&quot;, c(budget_year, project_number, program_element_number, row_num), 
          sep = &quot;_&quot;, na.rm = TRUE, remove = TRUE ) %&gt;%
    filter(!is.na(emerging_tech)) %&gt;% 
    mutate(text = str_replace_all(text, regex(&quot;\\W+&quot;), &quot; &quot;),
           text = str_remove_all(text, &quot;[\r\t\n]&quot;),
           emerging_tech = factor(emerging_tech, levels = c(&quot;0&quot;, &quot;1&quot;))) %&gt;%
    drop_na()



emerging_tech_raw %&gt;%
    count(emerging_tech) %&gt;% 
    ggplot(aes(x = emerging_tech, y = n, fill = emerging_tech)) +
    geom_col() +
    theme(legend.position = &quot;none&quot;)</code></pre>
<p><img src="index_files/figure-html/prep-1.png" width="672" /></p>
<p>As you can see, there’s some class imbalance in our emerging tech variable.</p>
<div id="modeling" class="section level1">
<h1>Modeling</h1>
<p>Next, we’ll split the data into training and testing data frames, and create 10 cross validation folds for resampling and tuning later on.</p>
<pre class="r"><code># emerging tech
set.seed(123)
emerging_split &lt;- initial_split(emerging_tech_raw) 
emerging_train &lt;- training(emerging_split)
emerging_test  &lt;- testing(emerging_split)

set.seed(234)
emerging_folds &lt;- vfold_cv(emerging_train, v = 10) </code></pre>
<p>We’ll try two different recipes, where the primary difference is that the second recipe creates text features from the data. While I don’t think most text features will be relevant to this data, I thought <em>maybe</em> the length of a program description and current/future year plans might be indicative of whether something was emerging tech or not (maybe emerging technologies require longer descriptions in order to explain what the technologies are?).</p>
<p>Otherwise, the recipes below mostly just take care of the ID column, tokenize the text column and set the max number of tokens to be tuned later on, use TF-IDF, normalize the predictors for using our lasso model, then upsample the dependent variable because of the class imbalance.</p>
<pre class="r"><code>## emerging tech
emerging_rec &lt;- recipe(emerging_tech ~ ., data = emerging_train) %&gt;% 
    update_role(id, new_role = &quot;id&quot;) %&gt;%
    step_tokenize(text) %&gt;%
    step_tokenfilter(text, max_tokens = tune::tune()) %&gt;%
    step_tfidf(text) %&gt;%
    step_normalize(recipes::all_predictors()) %&gt;%
    themis::step_upsample(emerging_tech)


emerging_rec_tf &lt;- recipe(emerging_tech ~ ., data = emerging_train) %&gt;% 
    update_role(id, new_role = &quot;id&quot;) %&gt;%
    step_mutate(text2 = text) %&gt;%
    step_textfeature(text2) %&gt;% 
    step_tokenize(text) %&gt;%
    step_tokenfilter(text, max_tokens = tune::tune()) %&gt;%
    step_tfidf(text) %&gt;%
    step_zv(recipes::all_predictors()) %&gt;%
    step_normalize(recipes::all_predictors()) %&gt;%
    themis::step_upsample(emerging_tech)</code></pre>
<p>We’ll try just a single model for demonstration – a lasso logistic regression. As <a href="https://smltar.com">SMLTAR</a> explains, a regularized classification model is well-suited for handling sparse data and avoiding over-fitting. We’ll tune the penalty values, too.</p>
<p>The recipe and model then go in a workflow.</p>
<pre class="r"><code>lasso_spec &lt;- logistic_reg(penalty = tune(), mixture = 1) %&gt;%
    set_mode(&quot;classification&quot;) %&gt;%
    set_engine(&quot;glmnet&quot;)

emerging_wf_lasso &lt;- workflow() %&gt;%
    add_recipe(emerging_rec) %&gt;%
    add_model(lasso_spec)

emerging_wf_lasso_tf &lt;- workflow() %&gt;%
    add_recipe(emerging_rec_tf) %&gt;%
    add_model(lasso_spec)</code></pre>
<p>I’ll then create a tuning grid:</p>
<pre class="r"><code>final_grid &lt;- grid_regular(penalty(range = c(-4, 0)),
                           max_tokens(range = c(500, 2000)),
                           levels = 6)</code></pre>
<p>And finally, we can fit the models. I’ll use parallel processing to sped things up.</p>
<pre class="r"><code>## lasso, no tf
all_cores &lt;- parallel::detectCores(logical = FALSE)
cl &lt;- parallel::makePSOCKcluster(all_cores)
doParallel::registerDoParallel(cl)


set.seed(1234)
emerging_lasso_rs &lt;- tune_grid(emerging_wf_lasso,
                      emerging_folds,
                      grid = final_grid,
                      metrics = metric_set(accuracy, roc_auc),
                      control = control_grid(save_pred = TRUE, pkgs = c(&#39;textrecipes&#39;)))


## lasso, with tf
set.seed(2345)
emerging_lasso_rs_tf &lt;- tune_grid(emerging_wf_lasso_tf,
                      emerging_folds,
                      grid = final_grid,
                      metrics = metric_set(accuracy, roc_auc),
                      control = control_grid(save_pred = TRUE, pkgs = c(&#39;textrecipes&#39;)))


# emerging_lasso_rs$.notes[[1]][[&quot;.notes&quot;]]</code></pre>
</div>
<div id="evaluate" class="section level1">
<h1>Evaluate</h1>
<pre class="r"><code>## emerging tech
collect_metrics(emerging_lasso_rs)</code></pre>
<pre><code>## # A tibble: 72 x 8
##     penalty max_tokens .metric  .estimator  mean     n std_err .config       
##       &lt;dbl&gt;      &lt;int&gt; &lt;chr&gt;    &lt;chr&gt;      &lt;dbl&gt; &lt;int&gt;   &lt;dbl&gt; &lt;chr&gt;         
##  1 0.0001          500 accuracy binary     0.695    10  0.0229 Recipe1_Model1
##  2 0.0001          500 roc_auc  binary     0.755    10  0.0220 Recipe1_Model1
##  3 0.000631        500 accuracy binary     0.695    10  0.0229 Recipe1_Model2
##  4 0.000631        500 roc_auc  binary     0.755    10  0.0220 Recipe1_Model2
##  5 0.00398         500 accuracy binary     0.708    10  0.0211 Recipe1_Model3
##  6 0.00398         500 roc_auc  binary     0.767    10  0.0240 Recipe1_Model3
##  7 0.0251          500 accuracy binary     0.714    10  0.0232 Recipe1_Model4
##  8 0.0251          500 roc_auc  binary     0.797    10  0.0269 Recipe1_Model4
##  9 0.158           500 accuracy binary     0.598    10  0.0427 Recipe1_Model5
## 10 0.158           500 roc_auc  binary     0.521    10  0.0117 Recipe1_Model5
## # ... with 62 more rows</code></pre>
<pre class="r"><code>show_best(emerging_lasso_rs, &quot;accuracy&quot;)</code></pre>
<pre><code>## # A tibble: 5 x 8
##    penalty max_tokens .metric  .estimator  mean     n std_err .config       
##      &lt;dbl&gt;      &lt;int&gt; &lt;chr&gt;    &lt;chr&gt;      &lt;dbl&gt; &lt;int&gt;   &lt;dbl&gt; &lt;chr&gt;         
## 1 0.00398        2000 accuracy binary     0.740    10  0.0285 Recipe6_Model3
## 2 0.0001         1700 accuracy binary     0.740    10  0.0317 Recipe5_Model1
## 3 0.000631       1700 accuracy binary     0.740    10  0.0317 Recipe5_Model2
## 4 0.0001         2000 accuracy binary     0.735    10  0.0270 Recipe6_Model1
## 5 0.000631       2000 accuracy binary     0.735    10  0.0270 Recipe6_Model2</code></pre>
<pre class="r"><code>show_best(emerging_lasso_rs, &quot;roc_auc&quot;)</code></pre>
<pre><code>## # A tibble: 5 x 8
##   penalty max_tokens .metric .estimator  mean     n std_err .config       
##     &lt;dbl&gt;      &lt;int&gt; &lt;chr&gt;   &lt;chr&gt;      &lt;dbl&gt; &lt;int&gt;   &lt;dbl&gt; &lt;chr&gt;         
## 1  0.0251        800 roc_auc binary     0.809    10  0.0266 Recipe2_Model4
## 2  0.0251        500 roc_auc binary     0.797    10  0.0269 Recipe1_Model4
## 3  0.0251       1100 roc_auc binary     0.791    10  0.0286 Recipe3_Model4
## 4  0.0251       1700 roc_auc binary     0.785    10  0.0357 Recipe5_Model4
## 5  0.0251       2000 roc_auc binary     0.783    10  0.0324 Recipe6_Model4</code></pre>
<pre class="r"><code>lasso_pred &lt;- collect_predictions(emerging_lasso_rs)


collect_metrics(emerging_lasso_rs_tf)</code></pre>
<pre><code>## # A tibble: 72 x 8
##     penalty max_tokens .metric  .estimator  mean     n std_err .config       
##       &lt;dbl&gt;      &lt;int&gt; &lt;chr&gt;    &lt;chr&gt;      &lt;dbl&gt; &lt;int&gt;   &lt;dbl&gt; &lt;chr&gt;         
##  1 0.0001          500 accuracy binary     0.697    10  0.0279 Recipe1_Model1
##  2 0.0001          500 roc_auc  binary     0.747    10  0.0298 Recipe1_Model1
##  3 0.000631        500 accuracy binary     0.697    10  0.0279 Recipe1_Model2
##  4 0.000631        500 roc_auc  binary     0.747    10  0.0298 Recipe1_Model2
##  5 0.00398         500 accuracy binary     0.716    10  0.0281 Recipe1_Model3
##  6 0.00398         500 roc_auc  binary     0.764    10  0.0315 Recipe1_Model3
##  7 0.0251          500 accuracy binary     0.746    10  0.0351 Recipe1_Model4
##  8 0.0251          500 roc_auc  binary     0.817    10  0.0334 Recipe1_Model4
##  9 0.158           500 accuracy binary     0.547    10  0.0489 Recipe1_Model5
## 10 0.158           500 roc_auc  binary     0.5      10  0      Recipe1_Model5
## # ... with 62 more rows</code></pre>
<pre class="r"><code>show_best(emerging_lasso_rs_tf, &quot;accuracy&quot;)</code></pre>
<pre><code>## # A tibble: 5 x 8
##   penalty max_tokens .metric  .estimator  mean     n std_err .config       
##     &lt;dbl&gt;      &lt;int&gt; &lt;chr&gt;    &lt;chr&gt;      &lt;dbl&gt; &lt;int&gt;   &lt;dbl&gt; &lt;chr&gt;         
## 1 0.0251         500 accuracy binary     0.746    10  0.0351 Recipe1_Model4
## 2 0.0251        1100 accuracy binary     0.740    10  0.0343 Recipe3_Model4
## 3 0.0251         800 accuracy binary     0.740    10  0.0350 Recipe2_Model4
## 4 0.00398       1700 accuracy binary     0.730    10  0.0294 Recipe5_Model3
## 5 0.0251        1400 accuracy binary     0.727    10  0.0361 Recipe4_Model4</code></pre>
<pre class="r"><code>show_best(emerging_lasso_rs_tf, &quot;roc_auc&quot;)</code></pre>
<pre><code>## # A tibble: 5 x 8
##   penalty max_tokens .metric .estimator  mean     n std_err .config       
##     &lt;dbl&gt;      &lt;int&gt; &lt;chr&gt;   &lt;chr&gt;      &lt;dbl&gt; &lt;int&gt;   &lt;dbl&gt; &lt;chr&gt;         
## 1  0.0251       1100 roc_auc binary     0.818    10  0.0376 Recipe3_Model4
## 2  0.0251        500 roc_auc binary     0.817    10  0.0334 Recipe1_Model4
## 3  0.0251       1700 roc_auc binary     0.805    10  0.0336 Recipe5_Model4
## 4  0.0251       1400 roc_auc binary     0.804    10  0.0343 Recipe4_Model4
## 5  0.0251        800 roc_auc binary     0.804    10  0.0364 Recipe2_Model4</code></pre>
<pre class="r"><code>lasso_pred_tf &lt;- collect_predictions(emerging_lasso_rs_tf)



lasso_pred %&gt;%
    group_by(id) %&gt;%
    roc_curve(truth = emerging_tech, .pred_0 ) %&gt;%
    autoplot()</code></pre>
<p><img src="index_files/figure-html/Evaluate-1.png" width="672" /></p>
<pre class="r"><code>lasso_pred_tf %&gt;%
    group_by(id) %&gt;%
    roc_curve(truth = emerging_tech, .pred_0 ) %&gt;%
    autoplot()</code></pre>
<p><img src="index_files/figure-html/Evaluate-2.png" width="672" /></p>
<pre class="r"><code>lasso_pred %&gt;%
    filter(id == &quot;Fold07&quot;) %&gt;%
    conf_mat(emerging_tech, .pred_class) %&gt;%
    autoplot(type = &quot;heatmap&quot;)</code></pre>
<p><img src="index_files/figure-html/Evaluate-3.png" width="672" /></p>
<pre class="r"><code>lasso_pred_tf %&gt;%
    filter(id == &quot;Fold09&quot;) %&gt;%
    conf_mat(emerging_tech, .pred_class) %&gt;%
    autoplot(type = &quot;heatmap&quot;)</code></pre>
<p><img src="index_files/figure-html/Evaluate-4.png" width="672" /></p>
<p>Overall the models did pretty well. The best non-text features model had a mean accuracy of 74% and ROC AUC of 80.8%, while the text features model had a slightly higher accuracy of 74.5% and ROC AUC of 81.8%. There was more variation between the folds in the text features model.</p>
<p>The confusion matrices are interesting. The non-TF model has a tougher time with distinguishing true negatives from false positives, while the text features model seems to default more towards classifying things as non-emerging tech programs, and as a result it has a higher false negative rate than we’d like.</p>
<p>So our choice of model would really come down to whether we’d like to be more conservative or not with classifying programs as emerging tech. If we go with the non-TF option, we’d have to be OK with potentially under-counting the emerging tech programs.</p>
</div>
<div id="finalize" class="section level1">
<h1>Finalize</h1>
<p>I’ll stick with the option using text features here. I’ll finalize my workflow based on the best ROC AUC, then calculate variable importance using the VIP package, the do a final fit on the test data.</p>
<pre class="r"><code>library(vip)

best_roc &lt;- select_best(emerging_lasso_rs_tf, &quot;roc_auc&quot;)


lasso_wf_final &lt;- finalize_workflow(emerging_wf_lasso_tf, best_roc)

lasso_wf_final %&gt;%
    fit(emerging_train) %&gt;%
    pull_workflow_fit() %&gt;%
    vi(lambda = best_roc$penalty) %&gt;%
    slice_head(n = 25) %&gt;% 
    mutate(Importance = abs(Importance),
           Variable = str_remove(Variable, &quot;tfidf_text_&quot;),
           Variable = fct_reorder(Variable, Importance)) %&gt;%
    ggplot(aes(Importance, Variable, fill = Sign)) + 
    geom_col() +
    theme_classic() + 
    labs(x = NULL, y = NULL)</code></pre>
<p><img src="index_files/figure-html/unnamed-chunk-2-1.png" width="672" /></p>
<pre class="r"><code>final_res &lt;- lasso_wf_final %&gt;%
    last_fit(emerging_split, metrics = metric_set(accuracy, roc_auc))</code></pre>
<p>Ok, so even though our model included text features, none of the text features variables ended up in the top 25 variables – instead we’ve got words like unmanned, modeling, sensor, machine, learning, etc. It’s interesting that many of these words seem to be related to AI - like machine learning, and modeling, while some are related to hypersonics – scramjet and thrust, and some are related to unmanned systems – unmanned, automated. Also interesting is that approaches was the third-most valuable term. Let’s see some descriptions of how approaches is used:</p>
<pre class="r"><code>set.seed(123)
emerging_train %&gt;%
    filter(str_detect(text, &quot;approaches&quot;),
           emerging_tech == 1) %&gt;%
    select(text) %&gt;%
    sample_n(5) </code></pre>
<p>So, we’ve got descriptions like:</p>
<ul>
<li>“…Activities will also develop and mature methods and technological <strong>approaches</strong> for environmental sensing and prediction for the maritime battlespace…”</li>
<li>“…Algorithm design and software engineering <strong>approaches</strong> are investigated to effectively partition and use binary processing cores to reduce time to solution for Army relevant problems…”</li>
<li>“Basic Research in Chemical Sciences Basic research to achieve advanced energy control improved threat detection and novel responsive materials for Soldier protection Research efforts will lead to light weight reliable compact power sources more effective lower vulnerability propellants and explosives for tailored precision strikes with minimum collateral damage new <strong>approaches</strong> for shielding the Soldier and Army platforms from ballistic chemical and biological threats and reducing signatures for identification by the enemy and advance warning of explosive chemical and biological weapons and dangerous industrial chemicals.”</li>
</ul>
</div>
<div id="finalize-1" class="section level1">
<h1>Finalize</h1>
<pre class="r"><code>collect_metrics(final_res)

final_res %&gt;% 
    collect_predictions() %&gt;%
    conf_mat(emerging_tech, .pred_class) %&gt;%
    autoplot(type = &quot;heatmap&quot;)</code></pre>
<p>Great! So it looks like we didn’t overfit – in fact, our accuracy increased. The confusion matrix also shows a better true positive rate as well. 78% accuracy and 80% ROC AUC is not too bad!</p>
</div>
<div id="lda" class="section level1">
<h1>LDA</h1>
<p>Finally, just for fun, we can do some topic modeling using LDA. Let’s see if it’s possible to model emerging tech topics by filtering for only programs labeled as emerging tech.</p>
<pre class="r"><code>## emerging tech
emerging_dtm &lt;- emerging_tech_raw %&gt;%
    filter(emerging_tech == 1) %&gt;% 
    unnest_tokens(word, text) %&gt;%
    anti_join(stop_words, by = &quot;word&quot;) %&gt;%
    count(id, word, sort = TRUE) %&gt;%
    cast_dtm(id, word, n)


emerging_lda_alt &lt;- LDA(emerging_dtm, k = 5, control = list(seed = 123))

emerging_topics &lt;- emerging_lda_alt %&gt;%
    tidy(matrix = &quot;beta&quot;)


emerging_top_terms &lt;- emerging_topics %&gt;%
    group_by(topic) %&gt;%
    top_n(10, abs(beta)) %&gt;%
    ungroup() %&gt;%
    arrange(topic, desc(beta))


emerging_top_terms %&gt;%
    mutate(term = reorder_within(term, beta, topic)) %&gt;%
    ggplot(aes(beta, term, fill = factor(topic))) +
    geom_col(show.legend = FALSE) +
    facet_wrap(~ topic, scales = &quot;free&quot;) +
    scale_y_reordered()</code></pre>
<p><img src="index_files/figure-html/unnamed-chunk-5-1.png" width="672" /></p>
<p>Let’s see what we have here:</p>
<ul>
<li><strong>Data</strong> is the clear first topic. Maybe that is AI-ish?</li>
<li>Systems development is the second topic, whcih includes <strong>unmanned and sensors development</strong>. So maybe this is related to unmanned systems?</li>
<li>The third topic is a little unclear to me – none of the top words distinguish it from the other topics at first glance.</li>
<li>The fourth topic also seems to be related to <strong>unmanned systems</strong>, although its top words include technology and the fiscal year (FY).</li>
<li>I’m guessing that the fifth topic has to do with <strong>basic research</strong>.</li>
</ul>
<p>Finally, the textrecipes package also provides a method for topic modeling using the step_lda function, although it doesn’t currently allow for extracting the per-topic-per-word probabilities (although this may be added later).</p>
<pre class="r"><code>emerging_lda_rec &lt;- recipe(~ ., data = emerging_tech_raw) %&gt;%
    update_role(id, new_role = &quot;id&quot;) %&gt;%
    update_role(emerging_tech, new_role = &quot;other&quot;) %&gt;%
    step_lda(text)


set.seed(123)
emerging_lda_prep &lt;- emerging_lda_rec %&gt;%
    prep()


emerging_lda &lt;- emerging_lda_prep %&gt;%
    juice()


emerging_lda2 &lt;- emerging_lda %&gt;%
    pivot_longer(lda_text_w1:lda_text_w10) %&gt;%
    group_by(id) %&gt;%
    top_n(1, value) %&gt;%
    select(id, top_topic = name) %&gt;%
    left_join(emerging_lda) %&gt;%
    left_join(emerging_tech_raw %&gt;% select(id, text))</code></pre>
</div>
